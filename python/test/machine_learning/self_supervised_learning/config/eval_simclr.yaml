# Configuration for SimCLR.

ssl_type: simclr
stage: evaluation

data:
  dataset: cifar10
  data_dir: data/cifar10
  image_shape: [32, 32, 3]

  batch_size: 512
  num_workers: 8

  train_transforms:
    random_resized_crop:
      size: [32, 32]
      scale: [0.2, 1.0]
    random_horizontal_flip:
      p: 0.5
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]
  test_transforms:
    #resize:
    #  size: [36, 36]
    center_crop:
      size: [32, 32]
    to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]

evaluation:
  epochs: 30

  # For classifier.
  user_defined_model:
    architecture:
      - module_type: linear
        in_features: 2048  # For ResNet50 or higher.
        out_features: 10
        bias: true
    output_dim: 10  # num_classes.
  #user_defined_model:
  #  architecture:
  #    # For MLP.
  #    - module_type: linear
  #      in_features: 2048  # For ResNet50 or higher.
  #      out_features: 256
  #      bias: true
  #    - module_type: batch_norm_1d
  #      num_features: 256
  #      eps: 1.0e-05
  #      momentum: 0.1
  #    - module_type: relu
  #      inplace: true
  #    - module_type: linear
  #      in_features: 256
  #      out_features: 10
  #      bias: true
  #  output_dim: 10  # num_classes.
  #predefined_model:
  #  model_name: linear  # {'linear', 'mlp'}.
  #  input_dim: 2048  # For ResNet50 or higher.

  optimizer:
    #sgd:
    #  lr: 0.1
    #  momentum: 0.9
    #  dampening: 0.0
    #  weight_decay: 1.0e-04
    #  nesterov: true
    adam:
      lr: 0.1
      betas: [0.9, 0.999]
      eps: 1.0e-08
      weight_decay: 0.0
      amsgrad: false
    #adadelta:
    #  lr: 0.1
    #  rho: 0.9
    #  eps: 1.0e-06
    #  weight_decay: 0.0
    #adagrad:
    #  lr: 0.1
    #  lr_decay: 0.0
    #  weight_decay: 0.0
    #  initial_accumulator_value: 0.0
    #  eps: 1.0e-10
    #rmsprop:
    #  lr: 0.1
    #  alpha: 0.99
    #  eps: 1.0e-08
    #  weight_decay: 0.0
    #  momentum: 0.0
    #  centered: false

  lr_scheduler:
    #step:
    #  step_size: 10
    #  gamma: 0.1
    #multi_step:
    #  milestones: [10, 20]
    #  gamma: 0.1
    #cosine_annealing:
    #  T_max: null  # epochs.
    #  eta_min: 0.0
    cosine_warmup:
      T_max: null  # epochs.
      T_warmup: 5
    #cosine_restart:
    #  T_0: 15
    #  T_mult: 1
    #  T_up: 5
    #  eta_max: 0.1  # eta_max > eta_init or eta_min.
    #  gamma: 1.0
    #noam:  # For transformer. Step-based.
    #  dim_feature: 256  # d_model.
    #  warmup_steps: 2000
    #  factor: 1.0
    #  epoch_based: false
