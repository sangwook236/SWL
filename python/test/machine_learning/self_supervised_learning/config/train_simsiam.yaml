# Configuration for SimSiam.

ssl_type: simsiam
stage: training

data:
  dataset: cifar10
  data_dir: data/cifar10
  image_shape: [32, 32, 3]

  batch_size: 512
  num_workers: 8

  ssl_transforms:
    # Data augmentation for SimCLR.
    random_resized_crop:
      size: [32, 32]
      scale: [0.08, 1.0]
      #scale: [0.2, 1.0]
    random_horizontal_flip:
      p: 0.5
    color_jitter:
      brightness: 0.8
      contrast: 0.8
      saturation: 0.8
      hue: 0.2
      #brightness: 0.4
      #contrast: 0.4
      #saturation: 0.4
      #hue: 0.1
      random_apply:
        p: 0.8
    random_grayscale:
      p: 0.2
    gaussian_blur:
      kernel_size: [3, 3]
      sigma: [0.1, 2.0]
      random_apply:
        p: 0.5
    #to_tensor:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std: [0.2470, 0.2435, 0.2616]
  train_transforms:
    #random_resized_crop:
    #  size: [32, 32]
    #  scale: [0.2, 1.0]
    #random_horizontal_flip:
    #  p: 0.5
    to_tensor:
    #normalize:
    #  mean: [0.4914, 0.4822, 0.4465]
    #  std: [0.2470, 0.2435, 0.2616]
  test_transforms:
    ##resize:
    ##  size: [36, 36]
    #center_crop:
    #  size: [32, 32]
    to_tensor:
    #normalize:
    #  mean: [0.4914, 0.4822, 0.4465]
    #  std: [0.2470, 0.2435, 0.2616]

model:
  # For encoder.
  pretrained_model:
    resnet50:
      feature_layer: avgpool
      feature_dim: 2048
      pretrained: true

  #projector_input_dim: 2048  # projector_input_dim = encoder_feature_dim.
  projector_hidden_dim: 4096
  projector_output_dim: 256
  #predictor_input_dim: 256  # predictor_input_dim = projector_output_dim.
  predictor_hidden_dim: 4096
  predictor_output_dim: 256

training:
  epochs: 100

  is_all_model_params_optimized: true

  #max_gradient_norm: 20.0
  max_gradient_norm: null  # No gradient clipping.
  swa: true

  optimizer:
    #sgd:
    #  lr: 0.1
    #  momentum: 0.9
    #  dampening: 0.0
    #  weight_decay: 1.0e-04
    #  nesterov: true
    adam:
      lr: 0.1
      betas: [0.9, 0.999]
      eps: 1.0e-08
      weight_decay: 0.0
      amsgrad: false
    #adadelta:
    #  lr: 0.1
    #  rho: 0.9
    #  eps: 1.0e-06
    #  weight_decay: 0.0
    #adagrad:
    #  lr: 0.1
    #  lr_decay: 0.0
    #  weight_decay: 0.0
    #  initial_accumulator_value: 0.0
    #  eps: 1.0e-10
    #rmsprop:
    #  lr: 0.1
    #  alpha: 0.99
    #  eps: 1.0e-08
    #  weight_decay: 0.0
    #  momentum: 0.0
    #  centered: false

  lr_scheduler:
    #step:
    #  step_size: 20
    #  gamma: 0.1
    #multi_step:
    #  milestones: [20, 40, 60, 80]
    #  gamma: 0.1
    #cosine_annealing:
    #  T_max: null  # epochs.
    #  eta_min: 0.0
    cosine_warmup:
      T_max: null  # epochs.
      T_warmup: 10
    #cosine_restart:
    #  T_0: 50
    #  T_mult: 1
    #  T_up: 10
    #  eta_max: 0.1  # eta_max > eta_init or eta_min.
    #  gamma: 1.0
    #noam:  # For transformer. Step-based.
    #  dim_feature: 256  # d_model.
    #  warmup_steps: 2000
    #  factor: 1.0
    #  epoch_based: false
